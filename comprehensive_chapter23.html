<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>How Multimodal LLMs â€œSeeâ€ (Vision + Text) - Comprehensive LLM Guide</title>
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="container">
        <header class="header">
            <h1>ğŸ‘ï¸ How Multimodal LLMs â€œSeeâ€</h1>
            <p class="subtitle">Chapter 23 of 26 - Comprehensive Guide</p>
        </header>

        <!-- Mobile Menu Toggle -->
        <button class="mobile-menu-toggle" onclick="toggleMobileMenu()" aria-label="Toggle menu">â˜°</button>

        <nav class="nav-sidebar" id="nav-sidebar">
            <ul>
                <li><a href="comprehensive_index.html">ğŸ  Home</a></li>
                <li><a href="comprehensive_chapter1.html">ğŸ¤– Chapter 1: Introduction to AI</a></li>
                <li><a href="comprehensive_chapter2.html">ğŸ“Š Chapter 2: Machine Learning</a></li>
                <li><a href="comprehensive_chapter3.html">ğŸ§  Chapter 3: Deep Learning</a></li>
                <li><a href="comprehensive_chapter4.html">ğŸ”— Chapter 4: Neural Networks</a></li>
                <li><a href="comprehensive_chapter5.html">ğŸ’¬ Chapter 5: NLP Evolution</a></li>
                <li><a href="comprehensive_chapter6.html">âš¡ Chapter 6: Transformers</a></li>
                <li><a href="comprehensive_chapter7.html">ğŸ“ Chapter 7: LLM Training</a></li>
                <li><a href="comprehensive_chapter8.html">ğŸ—ï¸ Chapter 8: LLM Architecture</a></li>
                <li><a href="comprehensive_chapter9.html">ğŸ”„ Chapter 9: Query Processing</a></li>
                <li><a href="comprehensive_chapter10.html">ğŸ‘ï¸ Chapter 10: Attention</a></li>
                <li><a href="comprehensive_chapter11.html">ğŸ“š Chapter 11: Training Data</a></li>
                <li><a href="comprehensive_chapter12.html">ğŸ¯ Chapter 12: Fine-tuning</a></li>
                <li><a href="comprehensive_chapter13.html">âš™ï¸ Chapter 13: Inference</a></li>
                <li><a href="comprehensive_chapter14.html">ğŸ“ˆ Chapter 14: Evolution</a></li>
                <li><a href="comprehensive_chapter15.html">ğŸš€ Chapter 15: Applications</a></li>
                <li><a href="comprehensive_chapter16.html">ğŸ”¤ Chapter 16: Tokenization</a></li>
                <li><a href="comprehensive_chapter17.html">ğŸ§® Chapter 17: Embeddings</a></li>
                <li><a href="comprehensive_chapter18.html">ğŸ”— Chapter 18: Tokenization vs Embeddings</a></li>
                <li><a href="comprehensive_chapter19.html">ğŸ­ Chapter 19: End-to-End LLM Lifecycle</a></li>
                <li><a href="comprehensive_chapter20.html">ğŸ² Chapter 20: How LLMs Generate Text</a></li>
                <li><a href="comprehensive_chapter21.html">ğŸ§  Chapter 21: How LLMs Understand Meaning</a></li>
                <li><a href="comprehensive_chapter22.html">ğŸ§ª Chapter 22: Training Recipe (Step-by-Step)</a></li>
                <li><a href="comprehensive_chapter23.html" class="active">ğŸ‘ï¸ Chapter 23: How Multimodal LLMs â€œSeeâ€</a></li>
                <li><a href="comprehensive_chapter24.html">ğŸ—„ï¸ Chapter 24: NL2SQL Deep Dive</a></li>
            </ul>
        </nav>

        <main class="main-content">
            <div class="chapter-header">
                <h1>ğŸ‘ï¸ Chapter 23: How Multimodal LLMs â€œSeeâ€ Images</h1>
                <p>How images become tokens/embeddings, how vision and language are fused, and what â€œseeing like humansâ€ really means (and doesnâ€™t).</p>
            </div>

            <div class="chapter-nav">
                <a href="comprehensive_chapter22.html">â† Previous Chapter</a>
                <a href="comprehensive_index.html">ğŸ  Home</a>
                <a href="comprehensive_chapter24.html">Next Chapter â†’</a>
            </div>

            <div class="section">
                <h2>Important Clarification: LLMs Donâ€™t See Like Humans</h2>
                <p><strong>They donâ€™t have eyes or a 3D world model.</strong> They process images as arrays of numbers (pixels) and learn statistical patterns that correlate with language. The result can look like â€œseeing,â€ but itâ€™s pattern recognition + reasoning over learned representations.</p>

                <div class="example-box warning-box">
                    <h4>âš ï¸ What â€œseeingâ€ means in practice</h4>
                    <p>The model can often: describe objects, read some text, compare diagrams, spot inconsistencies. But it can fail on tiny details, counting, or unusual viewpoints.</p>
                </div>

                <h2>Step 1: Image â†’ Patches (like tokens for vision)</h2>
                <p>A common approach (Vision Transformer style) splits an image into fixed-size patches (e.g., 16Ã—16 pixels). Each patch becomes a vector embedding, similar to a token embedding in text.</p>

                <div class="diagram-section">
                    <div class="diagram-explanation">
                        <h4>ğŸ§© Patch embeddings (concept)</h4>
                        <p>Text has tokens; vision has patches.</p>
                    </div>
                    <div class="diagram-container">
                        <div class="mermaid">
flowchart LR
    A[Image] --> B[Split into patches<br/>16x16 blocks]
    B --> C[Flatten each patch<br/>pixels â†’ vector]
    C --> D[Linear projection<br/>to embedding dim]
    D --> E[Sequence of patch embeddings<br/>(vision tokens)]

    style A fill:#e3f2fd
    style E fill:#e8f5e9
                        </div>
                    </div>
                </div>

                <h2>Step 2: Vision Encoder Builds Higher-Level Features</h2>
                <p>The vision encoder (often a ViT or CNN+Transformer) uses attention to combine patches into object/scene-level representations.</p>

                <div class="diagram-section">
                    <div class="diagram-container">
                        <div class="mermaid">
graph TD
    P[Patch embeddings] --> V[Vision Encoder<br/>self-attention layers]
    V --> F[Visual features<br/>objects, layout, text regions]

    style P fill:#e3f2fd
    style V fill:#fff3e0
    style F fill:#e8f5e9
                        </div>
                    </div>
                </div>

                <h2>Step 3: Fuse Vision + Language (How the model answers about images)</h2>
                <p>There are several architectures, but the high-level story is the same: the model produces a joint representation where text tokens can attend to image features.</p>

                <div class="diagram-section">
                    <div class="diagram-explanation">
                        <h4>ğŸ”— Typical multimodal fusion</h4>
                        <p>Vision features become part of the â€œcontextâ€ that the language model attends to.</p>
                    </div>
                    <div class="diagram-container">
                        <div class="mermaid">
flowchart TD
    Img[Image] --> VE[Vision Encoder]
    VE --> VF[Visual embeddings]
    Txt[Text prompt] --> TE[Text tokenization + embeddings]
    TE --> LM[Language Model (Transformer)]
    VF --> LM
    LM --> Out[Generated answer tokens]

    style Img fill:#e3f2fd
    style VF fill:#fff3e0
    style Out fill:#e8f5e9
                        </div>
                    </div>
                </div>

                <h2>How it â€œreadsâ€ text in images (OCR-like behavior)</h2>
                <p>Many multimodal models learn to interpret text regions, sometimes using explicit OCR modules, sometimes end-to-end. Performance varies with font, rotation, blur, and resolution.</p>
                <div class="example-box">
                    <h4>ğŸ“Œ Example (diagram + question)</h4>
                    <pre><code>Image: flowchart with labels "Tokenize â†’ Embed â†’ Attention â†’ Output"
Question: "What comes after Embed?"
Model uses visual-text features and layout relationships to answer: "Attention".</code></pre>
                </div>

                <h2>Limitations (very important for an honest â€œbest prizeâ€ talk)</h2>
                <ul>
                    <li><strong>Fine detail</strong>: tiny text or small objects can be missed.</li>
                    <li><strong>Counting</strong>: exact counts can fail (e.g., â€œhow many pills?â€).</li>
                    <li><strong>Visual illusions</strong>: unusual perspectives can confuse.</li>
                    <li><strong>Grounding</strong>: it can describe plausible things not present (visual hallucination).</li>
                </ul>

                <div class="example-box error-box">
                    <h4>âŒ Visual hallucination example</h4>
                    <p>If the image is blurry, the model may guess â€œa red carâ€ because thatâ€™s a common completionâ€”even if the object is not a car.</p>
                </div>

                <h2>Final Summary</h2>
                <ol>
                    <li><strong>Images become â€œvision tokensâ€</strong> (patch embeddings or similar).</li>
                    <li><strong>A vision encoder</strong> converts patches into higher-level features.</li>
                    <li><strong>Fusion</strong> lets the language model attend to visual features and generate text token-by-token.</li>
                    <li><strong>Itâ€™s powerful but not human vision</strong>: detail/counting/grounding remain challenging.</li>
                </ol>
            </div>

            <div class="chapter-nav">
                <a href="comprehensive_chapter22.html">â† Previous Chapter</a>
                <a href="comprehensive_index.html">ğŸ  Home</a>
                <a href="comprehensive_chapter24.html">Next Chapter â†’</a>
            </div>
        </main>

        <footer class="footer">
            <p>Â© 2024 NL2SQL Project - Comprehensive LLM Learning Guide</p>
            <p>Chapter 23 of 26</p>
        </footer>
    </div>

    <script>
        mermaid.initialize({
            startOnLoad: true,
            theme: 'default',
            flowchart: { useMaxWidth: true, htmlLabels: true }
        });

        // Mobile menu toggle
        function toggleMobileMenu() {
            const nav = document.getElementById('nav-sidebar');
            if (nav) nav.classList.toggle('open');
        }

        // Close mobile menu when clicking outside
        document.addEventListener('click', function(event) {
            const nav = document.getElementById('nav-sidebar');
            const toggle = document.querySelector('.mobile-menu-toggle');

            if (nav && toggle &&
                !nav.contains(event.target) &&
                !toggle.contains(event.target) &&
                nav.classList.contains('open')) {
                nav.classList.remove('open');
            }
        });

        // Close mobile menu when clicking a link
        document.querySelectorAll('.nav-sidebar a').forEach(link => {
            link.addEventListener('click', function() {
                const nav = document.getElementById('nav-sidebar');
                if (nav && window.innerWidth <= 768) {
                    nav.classList.remove('open');
                }
            });
        });
    </script>
</body>
</html>

