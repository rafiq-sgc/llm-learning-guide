<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Learning Basics - Comprehensive LLM Guide</title>
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="container">
        <header class="header">
            <h1>ğŸ§  Deep Learning Basics</h1>
            <p class="subtitle">Chapter 3 of 15 - Comprehensive Guide</p>
        </header>

        
    <!-- Mobile Menu Toggle -->
    <button class="mobile-menu-toggle" onclick="toggleMobileMenu()" aria-label="Toggle menu">â˜°</button>

    <nav class="nav-sidebar" id="nav-sidebar">
        <ul>
            <li><a href="comprehensive_index.html">ğŸ  Home</a></li>
            <li><a href="comprehensive_chapter1.html">ğŸ¤– Chapter 1: Introduction to AI</a></li>
            <li><a href="comprehensive_chapter2.html">ğŸ“Š Chapter 2: Machine Learning</a></li>
            <li><a href="comprehensive_chapter3.html">ğŸ§  Chapter 3: Deep Learning</a></li>
            <li><a href="comprehensive_chapter4.html">ğŸ”— Chapter 4: Neural Networks</a></li>
            <li><a href="comprehensive_chapter5.html">ğŸ’¬ Chapter 5: NLP Evolution</a></li>
            <li><a href="comprehensive_chapter6.html">âš¡ Chapter 6: Transformers</a></li>
            <li><a href="comprehensive_chapter7.html">ğŸ“ Chapter 7: LLM Training</a></li>
            <li><a href="comprehensive_chapter8.html">ğŸ—ï¸ Chapter 8: LLM Architecture</a></li>
            <li><a href="comprehensive_chapter9.html">ğŸ”„ Chapter 9: Query Processing</a></li>
            <li><a href="comprehensive_chapter10.html">ğŸ‘ï¸ Chapter 10: Attention</a></li>
            <li><a href="comprehensive_chapter11.html">ğŸ“š Chapter 11: Training Data</a></li>
            <li><a href="comprehensive_chapter12.html">ğŸ¯ Chapter 12: Fine-tuning</a></li>
            <li><a href="comprehensive_chapter13.html">âš™ï¸ Chapter 13: Inference</a></li>
            <li><a href="comprehensive_chapter14.html">ğŸ“ˆ Chapter 14: Evolution</a></li>
            <li><a href="comprehensive_chapter15.html">ğŸš€ Chapter 15: Applications</a></li>
        </ul>
    </nav>

        <main class="main-content">
            <div class="chapter-header">
                <h1>ğŸ§  Chapter 3: Deep Learning Basics</h1>
                <p>Comprehensive Learning Guide - Detailed Presentation Material</p>
            </div>

            <div class="chapter-nav"><a href="comprehensive_chapter2.html">â† Previous Chapter</a><a href="comprehensive_index.html">ğŸ  Home</a><a href="comprehensive_chapter4.html">Next Chapter â†’</a></div>

            <div class="section">
                <h3>What is Deep Learning?</h3>
<p><strong>Deep Learning</strong> is a subset of machine learning that uses neural networks with multiple layers (hence "deep") to learn representations of data.</p>
<h3>Why Deep Learning?</h3>
<p><strong>Traditional ML:</strong></p>
<ul>
<li>Requires feature engineering (manual)</li>
<li>Limited to simple patterns</li>
<li>Performance plateaus</li>
</ul>
<p><strong>Deep Learning:</strong></p>
<ul>
<li>Automatic feature learning</li>
<li>Can learn complex patterns</li>
<li>Scales with data and compute</li>
</ul>
<h3>Neural Network Basics</h3>
<p><strong>Perceptron (Simplest Neural Network):</strong></p>

        <div class="diagram-section">
            <div class="diagram-explanation">
                <h4>ğŸ“Š Understanding Perceptron - The Simplest Neural Network:</h4>
                <p><strong>What is a Perceptron?</strong> A perceptron is the simplest possible neural network - just one neuron. It's the building block of all neural networks. This diagram shows how it processes inputs to produce an output.</p>
                
                <div class="step-by-step">
                    <h5>ğŸ” Step-by-Step Breakdown:</h5>
                    <ol>
                        <li><strong>Inputs (X1, X2, X3):</strong>
                            <ul>
                                <li>Three input features</li>
                                <li>Example: X1 = temperature, X2 = humidity, X3 = pressure</li>
                                <li>These are the raw data values</li>
                            </ul>
                        </li>
                        
                        <li><strong>Weights (w1, w2, w3):</strong>
                            <ul>
                                <li>Each input has a weight (importance)</li>
                                <li>w1 = weight for X1, w2 = weight for X2, w3 = weight for X3</li>
                                <li>Learned during training</li>
                                <li>Example: If w1 is large, X1 is very important</li>
                            </ul>
                        </li>
                        
                        <li><strong>Weighted Sum (Î£ - Orange):</strong>
                            <ul>
                                <li>Calculate: w1Ã—X1 + w2Ã—X2 + w3Ã—X3 + bias</li>
                                <li>Multiplies each input by its weight</li>
                                <li>Adds all weighted inputs together</li>
                                <li>Adds bias (constant term)</li>
                                <li>Result: A single number</li>
                            </ul>
                        </li>
                        
                        <li><strong>Bias (B):</strong>
                            <ul>
                                <li>Constant value added to the sum</li>
                                <li>Allows the model to shift the decision boundary</li>
                                <li>Like the intercept in linear regression</li>
                                <li>Also learned during training</li>
                            </ul>
                        </li>
                        
                        <li><strong>Activation Function (Green):</strong>
                            <ul>
                                <li>Applies a function to the weighted sum</li>
                                <li>Common functions:
                                    <ul>
                                        <li><strong>Sigmoid:</strong> Outputs 0 to 1 (probability-like)</li>
                                        <li><strong>ReLU:</strong> Outputs 0 or positive (most common)</li>
                                        <li><strong>Tanh:</strong> Outputs -1 to 1</li>
                                    </ul>
                                </li>
                                <li>Adds non-linearity (enables learning complex patterns)</li>
                                <li>Without activation, it's just linear regression</li>
                            </ul>
                        </li>
                        
                        <li><strong>Output (Y):</strong>
                            <ul>
                                <li>Final prediction or classification</li>
                                <li>Example: 0.8 (80% probability of rain)</li>
                                <li>This is what the perceptron predicts</li>
                            </ul>
                        </li>
                    </ol>
                </div>
                
                <p><strong>ğŸ’¡ Key Takeaway:</strong> A perceptron takes multiple inputs, weights them, sums them with a bias, applies an activation function, and produces an output. This simple process, when repeated in layers, creates powerful neural networks.</p>
                
                <p><strong>ğŸ¯ Real-World Example:</strong> Predicting if it will rain:
                    <ul>
                        <li>Inputs: Temperature (X1=25Â°C), Humidity (X2=80%), Pressure (X3=1013 hPa)</li>
                        <li>Weights: w1=0.1, w2=0.8, w3=-0.2 (humidity is most important)</li>
                        <li>Sum: 0.1Ã—25 + 0.8Ã—80 + (-0.2)Ã—1013 + bias = -120</li>
                        <li>Activation (Sigmoid): f(-120) = 0.0 (very low probability)</li>
                        <li>Wait, that doesn't make sense... Let's recalculate with proper weights!</li>
                    </ul>
                </p>
                
                <p><strong>ğŸ“š Mathematical Formula:</strong> <code>y = f(w1Ã—x1 + w2Ã—x2 + w3Ã—x3 + bias)</code></p>
            </div>
            
            <div class="diagram-container">
                <div class="mermaid">
graph LR
    X1[x1] -->|w1| Sum[Î£]
    X2[x2] -->|w2| Sum
    X3[x3] -->|w3| Sum
    B[bias] --> Sum
    Sum --> Act[Activation<br/>Function]
    Act --> Y[Output]
    
    style Sum fill:#fff3e0
    style Act fill:#e8f5e9
                </div>
            </div>
        </div>
        
<p><strong>Mathematical Representation:</strong></p>
<pre><code>y = f(w1*x1 + w2*x2 + w3*x3 + bias)
<p></code></pre></p>
<p><strong>Activation Functions:</strong></p>
<ul>
<li><strong>Sigmoid</strong>: S-shaped curve, outputs 0-1</li>
<li><strong>ReLU</strong>: Rectified Linear Unit, outputs 0 or positive</li>
<li><strong>Tanh</strong>: Hyperbolic tangent, outputs -1 to 1</li>
</ul>
<h3>Multi-Layer Neural Network</h3>

        <div class="diagram-section">
            <div class="diagram-explanation">
                <h4>ğŸ“Š Understanding Multi-Layer Neural Network:</h4>
                <p><strong>What is a Multi-Layer Neural Network?</strong> This diagram shows a neural network with multiple layers - this is what makes it "deep". Each layer processes the previous layer's output, building increasingly complex representations.</p>
                
                <div class="step-by-step">
                    <h5>ğŸ” Step-by-Step Breakdown:</h5>
                    <ol>
                        <li><strong>Input Layer (Blue):</strong>
                            <ul>
                                <li>Receives raw input data</li>
                                <li>Example: Image pixels, text features, sensor readings</li>
                                <li>Each node represents one input feature</li>
                                <li>No computation here - just passes data to next layer</li>
                            </ul>
                        </li>
                        
                        <li><strong>Hidden Layer 1 (Orange):</strong>
                            <ul>
                                <li>First layer of computation</li>
                                <li>Learns basic patterns from input</li>
                                <li>Example: In image recognition, might detect edges</li>
                                <li>Each neuron in this layer receives inputs from all input neurons</li>
                                <li>Applies weights, sums, and activation function</li>
                            </ul>
                        </li>
                        
                        <li><strong>Hidden Layer 2 (Orange):</strong>
                            <ul>
                                <li>Second layer of computation</li>
                                <li>Learns patterns from Layer 1's patterns</li>
                                <li>More abstract representations</li>
                                <li>Example: In image recognition, might detect shapes (combinations of edges)</li>
                                <li>Each neuron receives inputs from all Layer 1 neurons</li>
                            </ul>
                        </li>
                        
                        <li><strong>Hidden Layer 3 (Orange):</strong>
                            <ul>
                                <li>Third layer of computation</li>
                                <li>Learns even more complex patterns</li>
                                <li>Example: In image recognition, might detect objects (combinations of shapes)</li>
                                <li>Each layer builds on previous layers</li>
                            </ul>
                        </li>
                        
                        <li><strong>Output Layer (Green):</strong>
                            <ul>
                                <li>Final layer produces the prediction</li>
                                <li>Number of neurons = number of possible outputs</li>
                                <li>Example: For classification, might have 10 neurons (for 10 classes)</li>
                                <li>Each neuron represents probability of that class</li>
                            </ul>
                        </li>
                    </ol>
                </div>
                
                <p><strong>ğŸ’¡ Key Takeaway:</strong> Information flows from input â†’ hidden layers â†’ output. Each layer learns increasingly complex patterns. More layers = deeper network = can learn more complex patterns, but also harder to train.</p>
                
                <p><strong>ğŸ¯ Real-World Analogy:</strong> Like a factory assembly line:
                    <ul>
                        <li>Input: Raw materials</li>
                        <li>Layer 1: Basic components</li>
                        <li>Layer 2: Sub-assemblies</li>
                        <li>Layer 3: Complex assemblies</li>
                        <li>Output: Finished product</li>
                    </ul>
                </p>
                
                <p><strong>ğŸ”¢ Why Multiple Layers?</strong> Each layer can learn different levels of abstraction:
                    <ul>
                        <li>Layer 1: Simple features (edges, colors)</li>
                        <li>Layer 2: Combinations of features (shapes, textures)</li>
                        <li>Layer 3: Complex combinations (objects, patterns)</li>
                        <li>Output: Final decision (classification, prediction)</li>
                    </ul>
                </p>
            </div>
            
            <div class="diagram-container">
                <div class="mermaid">
graph TD
    Input[Input Layer] --> H1[Hidden Layer 1]
    H1 --> H2[Hidden Layer 2]
    H2 --> H3[Hidden Layer 3]
    H3 --> Output[Output Layer]
    
    style Input fill:#e3f2fd
    style H1 fill:#fff3e0
    style H2 fill:#fff3e0
    style H3 fill:#fff3e0
    style Output fill:#e8f5e9
                </div>
            </div>
        </div>
        
<p><strong>Key Concepts:</strong></p>
<ul>
<li><strong>Layers</strong>: Stack of neurons</li>
<li><strong>Weights</strong>: Connection strengths (learned parameters)</li>
<li><strong>Biases</strong>: Additional parameters</li>
<li><strong>Forward Pass</strong>: Data flows from input to output</li>
<li><strong>Backward Pass</strong>: Error flows back to adjust weights (backpropagation)</li>
</ul>
<p>---</p>
            </div>

            <div class="chapter-nav"><a href="comprehensive_chapter2.html">â† Previous Chapter</a><a href="comprehensive_index.html">ğŸ  Home</a><a href="comprehensive_chapter4.html">Next Chapter â†’</a></div>
        </main>

        <footer class="footer">
            <p>Â© 2024 NL2SQL Project - Comprehensive LLM Learning Guide</p>
            <p>Chapter 3 of 15</p>
        </footer>
    </div>

    <script>
        mermaid.initialize({ 
            startOnLoad: true,
            theme: 'default',
            flowchart: { useMaxWidth: true, htmlLabels: true }
        });
        
        // Mobile menu toggle
        function toggleMobileMenu() {
            const nav = document.getElementById('nav-sidebar');
            if (nav) nav.classList.toggle('open');
        }

        // Close mobile menu when clicking outside
        document.addEventListener('click', function(event) {
            const nav = document.getElementById('nav-sidebar');
            const toggle = document.querySelector('.mobile-menu-toggle');
            
            if (nav && toggle && 
                !nav.contains(event.target) && 
                !toggle.contains(event.target) &&
                nav.classList.contains('open')) {
                nav.classList.remove('open');
            }
        });

        // Close mobile menu when clicking a link
        document.querySelectorAll('.nav-sidebar a').forEach(link => {
            link.addEventListener('click', function() {
                const nav = document.getElementById('nav-sidebar');
                if (nav && window.innerWidth <= 768) {
                    nav.classList.remove('open');
                }
            });
        });
    </script>
</body>
</html>