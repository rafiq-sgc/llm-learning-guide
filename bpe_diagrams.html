<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BPE Tokenization Diagrams</title>
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            max-width: 1400px;
            margin: 0 auto;
            padding: 20px;
            background: #f5f5f5;
        }
        .diagram-container {
            background: white;
            padding: 30px;
            margin: 30px 0;
            border-radius: 12px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        h1 {
            color: #2196F3;
            text-align: center;
            margin-bottom: 10px;
        }
        h2 {
            color: #4CAF50;
            border-bottom: 3px solid #4CAF50;
            padding-bottom: 10px;
            margin-top: 40px;
        }
        .description {
            background: #e3f2fd;
            padding: 15px;
            border-left: 4px solid #2196F3;
            margin: 20px 0;
            border-radius: 4px;
        }
        .mermaid {
            text-align: center;
            margin: 20px 0;
        }
    </style>
</head>
<body>
    <h1>üî¢ BPE (Byte Pair Encoding) Tokenization Diagrams</h1>
    <p style="text-align: center; color: #666; font-size: 1.1rem;">Visual representations of the BPE tokenization process</p>

    <!-- Diagram 1: Complete BPE Process Flow -->
    <div class="diagram-container">
        <h2>Diagram 1: Complete BPE Process Flow</h2>
        <div class="description">
            <strong>Overview:</strong> This diagram shows the complete BPE training process from start to finish, including how new text is tokenized after training.
        </div>
        <div class="mermaid">
            graph TD
                A[Start: Training Corpus] --> B[Step 1: Character Representation]
                B --> C["Words: low, lower, newest, widest<br/>Add end symbol: </w>"]
                C --> D["l o w </w><br/>l o w e r </w><br/>n e w e s t </w><br/>w i d e s t </w>"]
                D --> E[Step 2: Count Pair Frequencies]
                E --> F["('l','o') ‚Üí 2<br/>('o','w') ‚Üí 2<br/>('e','s') ‚Üí 2<br/>('s','t') ‚Üí 2"]
                F --> G[Step 3: Find Most Frequent Pair]
                G --> H["Most Frequent: ('o','w') ‚Üí 2"]
                H --> I[Step 4: Merge Pair]
                I --> J["Merge: ('o','w') ‚Üí 'ow'<br/>Result: l ow </w>, l ow e r </w>"]
                J --> K{More pairs<br/>to merge?}
                K -->|Yes| E
                K -->|No| L[Final Vocabulary]
                L --> M["Vocabulary:<br/>low, lower, newest, widest<br/>est, ow, es, ..."]
                M --> N[Step 5: Tokenize New Text]
                N --> O["Input: 'lowest'<br/>Tokenized: 'low' + 'est'"]
                
                style A fill:#e1f5ff
                style B fill:#fff3e0
                style E fill:#fff3e0
                style I fill:#fff3e0
                style L fill:#e8f5e9
                style O fill:#e8f5e9
        </div>
    </div>

    <!-- Diagram 2: Step-by-Step BPE Merging Process -->
    <div class="diagram-container">
        <h2>Diagram 2: Step-by-Step BPE Merging Process</h2>
        <div class="description">
            <strong>Overview:</strong> Visual representation of how words transform through each merging iteration.
        </div>
        <div class="mermaid">
            graph LR
                subgraph Step1["Step 1: Initial Characters"]
                    A1["l o w </w>"]
                    A2["l o w e r </w>"]
                    A3["n e w e s t </w>"]
                    A4["w i d e s t </w>"]
                end
                
                subgraph Step2["Step 2: Count Pairs"]
                    B1["('l','o') = 2"]
                    B2["('o','w') = 2 ‚≠ê"]
                    B3["('e','s') = 2"]
                    B4["('s','t') = 2"]
                end
                
                subgraph Step3["Step 3: Merge ('o','w') ‚Üí 'ow'"]
                    C1["l ow </w>"]
                    C2["l ow e r </w>"]
                    C3["n e w e s t </w>"]
                    C4["w i d e s t </w>"]
                end
                
                subgraph Step4["Step 4: Next Merges"]
                    D1["('e','s') ‚Üí 'es'"]
                    D2["('es','t') ‚Üí 'est'"]
                end
                
                subgraph Step5["Final Vocabulary"]
                    E1["low"]
                    E2["lower"]
                    E3["newest"]
                    E4["widest"]
                    E5["est"]
                    E6["ow"]
                end
                
                Step1 --> Step2
                Step2 --> Step3
                Step3 --> Step4
                Step4 --> Step5
                
                style Step2 fill:#fff3e0
                style Step3 fill:#e8f5e9
                style Step5 fill:#e1f5ff
        </div>
    </div>

    <!-- Diagram 3: BPE Training vs Tokenization -->
    <div class="diagram-container">
        <h2>Diagram 3: BPE Training vs Tokenization</h2>
        <div class="description">
            <strong>Overview:</strong> Shows the difference between the training phase (learning merges) and the tokenization phase (applying learned merges to new text).
        </div>
        <div class="mermaid">
            graph TB
                subgraph Training["üîµ Training Phase"]
                    T1[Training Corpus] --> T2[Initialize with Characters]
                    T2 --> T3[Count All Pairs]
                    T3 --> T4[Find Most Frequent]
                    T4 --> T5[Merge Pair]
                    T5 --> T6{Reached<br/>Vocabulary Size?}
                    T6 -->|No| T3
                    T6 -->|Yes| T7[Save Vocabulary]
                end
                
                subgraph Tokenization["üü¢ Tokenization Phase"]
                    I1[New Text Input] --> I2[Start with Characters]
                    I2 --> I3[Find Longest Match]
                    I3 --> I4[Apply Merged Pairs]
                    I4 --> I5{More<br/>Characters?}
                    I5 -->|Yes| I3
                    I5 -->|No| I6[Output Tokens]
                end
                
                T7 --> I1
                
                style Training fill:#e3f2fd
                style Tokenization fill:#e8f5e9
        </div>
    </div>

    <!-- Diagram 4: Detailed Example - Tokenizing "lowest" -->
    <div class="diagram-container">
        <h2>Diagram 4: Detailed Example - Tokenizing "lowest"</h2>
        <div class="description">
            <strong>Overview:</strong> Step-by-step process of how the word "lowest" (not seen in training) gets tokenized using learned merges.
        </div>
        <div class="mermaid">
            graph TD
                A["Input: 'lowest'<br/>(not in training data)"] --> B[Step 1: Character Split]
                B --> C["l o w e s t"]
                C --> D[Step 2: Apply Learned Merges]
                D --> E["Check: 'l' + 'o' ‚Üí 'lo'?<br/>No match in vocabulary"]
                E --> F["Check: 'o' + 'w' ‚Üí 'ow'?<br/>‚úÖ Found in vocabulary!"]
                F --> G["Merge: 'ow'<br/>Current: l ow e s t"]
                G --> H["Check: 'e' + 's' ‚Üí 'es'?<br/>‚úÖ Found in vocabulary!"]
                H --> I["Merge: 'es'<br/>Current: l ow es t"]
                I --> J["Check: 'es' + 't' ‚Üí 'est'?<br/>‚úÖ Found in vocabulary!"]
                J --> K["Merge: 'est'<br/>Current: l ow est"]
                K --> L["Check: 'l' + 'ow' ‚Üí 'low'?<br/>‚úÖ Found in vocabulary!"]
                L --> M["Final Merge: 'low'<br/>Current: low est"]
                M --> N["‚úÖ Final Tokens:<br/>['low', 'est']"]
                
                style A fill:#fff3e0
                style F fill:#c8e6c9
                style H fill:#c8e6c9
                style J fill:#c8e6c9
                style L fill:#c8e6c9
                style N fill:#4caf50
        </div>
    </div>

    <!-- Diagram 5: BPE Algorithm Flowchart -->
    <div class="diagram-container">
        <h2>Diagram 5: BPE Algorithm Flowchart</h2>
        <div class="description">
            <strong>Overview:</strong> High-level algorithm flowchart showing the iterative nature of BPE training.
        </div>
        <div class="mermaid">
            flowchart TD
                Start([Start BPE Training]) --> Init[Initialize Vocabulary<br/>with all characters]
                Init --> Corpus[Load Training Corpus]
                Corpus --> Split[Split words into characters<br/>Add end symbol </w>]
                Split --> Count[Count frequency of<br/>all adjacent pairs]
                Count --> Find[Find most frequent pair]
                Find --> Check{Reached target<br/>vocabulary size?}
                Check -->|No| Merge[Merge most frequent pair<br/>into single token]
                Merge --> Update[Update all occurrences<br/>in corpus]
                Update --> Count
                Check -->|Yes| Save[Save final vocabulary]
                Save --> End([End Training])
                
                style Start fill:#e1f5ff
                style Merge fill:#fff3e0
                style Save fill:#e8f5e9
                style End fill:#e1f5ff
        </div>
    </div>

    <!-- Diagram 6: Visual Representation of Merges -->
    <div class="diagram-container">
        <h2>Diagram 6: Visual Representation of Merges</h2>
        <div class="description">
            <strong>Overview:</strong> Shows how words evolve through multiple iterations of merging.
        </div>
        <div class="mermaid">
            graph TB
                subgraph Iteration1["Iteration 1: Initial State"]
                    I1A["l o w </w>"]
                    I1B["l o w e r </w>"]
                    I1C["n e w e s t </w>"]
                    I1D["w i d e s t </w>"]
                end
                
                subgraph Iteration2["Iteration 2: After merging ('o','w')"]
                    I2A["l ow </w>"]
                    I2B["l ow e r </w>"]
                    I2C["n e w e s t </w>"]
                    I2D["w i d e s t </w>"]
                end
                
                subgraph Iteration3["Iteration 3: After merging ('e','s')"]
                    I3A["l ow </w>"]
                    I3B["l ow e r </w>"]
                    I3C["n e w es t </w>"]
                    I3D["w i d es t </w>"]
                end
                
                subgraph Iteration4["Iteration 4: After merging ('es','t')"]
                    I4A["l ow </w>"]
                    I4B["l ow e r </w>"]
                    I4C["n e w est </w>"]
                    I4D["w i d est </w>"]
                end
                
                Iteration1 -->|Merge 'ow'| Iteration2
                Iteration2 -->|Merge 'es'| Iteration3
                Iteration3 -->|Merge 'est'| Iteration4
                
                style Iteration1 fill:#fff3e0
                style Iteration2 fill:#e3f2fd
                style Iteration3 fill:#e8f5e9
                style Iteration4 fill:#f3e5f5
        </div>
    </div>

    <!-- Diagram 7: Complete BPE Example with All Steps -->
    <div class="diagram-container">
        <h2>Diagram 7: Complete BPE Example - Sequence Diagram</h2>
        <div class="description">
            <strong>Overview:</strong> Sequence diagram showing the interaction between different components during BPE training and tokenization.
        </div>
        <div class="mermaid">
            sequenceDiagram
                participant Corpus as Training Corpus
                participant BPE as BPE Algorithm
                participant Vocab as Vocabulary
                participant Tokenizer as Tokenizer
                
                Note over Corpus: Words: low, lower,<br/>newest, widest
                
                Corpus->>BPE: Step 1: Character split
                BPE->>BPE: l o w </w><br/>l o w e r </w><br/>n e w e s t </w><br/>w i d e s t </w>
                
                BPE->>BPE: Step 2: Count pairs
                BPE->>BPE: ('o','w') = 2 (most frequent)
                
                BPE->>BPE: Step 3: Merge ('o','w') ‚Üí 'ow'
                BPE->>Vocab: Add 'ow' to vocabulary
                
                BPE->>BPE: Step 4: Repeat counting
                BPE->>BPE: ('e','s') = 2 (most frequent)
                BPE->>BPE: Merge ('e','s') ‚Üí 'es'
                BPE->>Vocab: Add 'es' to vocabulary
                
                BPE->>BPE: Continue merging...
                BPE->>Vocab: Final vocabulary ready
                
                Note over Tokenizer: New word: "lowest"
                Tokenizer->>Vocab: Look up merges
                Vocab->>Tokenizer: 'ow', 'es', 'est', 'low'
                Tokenizer->>Tokenizer: Apply merges greedily
                Tokenizer->>Tokenizer: Result: ['low', 'est']
        </div>
    </div>

    <div style="text-align: center; margin: 40px 0; padding: 20px; background: #e8f5e9; border-radius: 8px;">
        <h3 style="color: #4CAF50; margin-bottom: 10px;">üìù Usage Instructions</h3>
        <p style="color: #333;">Copy any mermaid diagram code from the markdown file (<code>bpe_tokenization_diagram.md</code>) and paste it into:</p>
        <ul style="text-align: left; display: inline-block; color: #333;">
            <li>HTML files with mermaid.js</li>
            <li>Markdown files with mermaid support</li>
            <li>Presentation tools that support mermaid</li>
            <li>Online mermaid editors (mermaid.live)</li>
        </ul>
    </div>

    <script>
        mermaid.initialize({ 
            startOnLoad: true,
            theme: 'default',
            flowchart: { useMaxWidth: true, htmlLabels: true }
        });
    </script>
</body>
</html>

